[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website every time before you want to upload changes."
  },
  {
    "objectID": "content/01_journal/02_statistics.html",
    "href": "content/01_journal/02_statistics.html",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Expected value of\n\nage: \\(E[age] = 33.471\\)\n\nincome: \\(E[income] = 3510.731\\)\n\n\n\nVariance of\n\nage: \\(Var[age] = 340.6078\\)\n\nincome: \\(Var[income] = 8625646\\)\n\n\n\nStandard deviation of\n\nage: \\(\\sigma = 18.45556\\)\n\nincome: \\(\\sigma = 2936.945\\)\n\n\n\n\n\n# Expected value of \"age\"\nE_age &lt;- mean(unlist(random_vars[\"age\"]))\n# Expected value of \"income\"\nE_income &lt;- mean(unlist(random_vars[\"income\"]))\n# Variance of \"age\"\nVar_age &lt;- var(random_vars[\"age\"])\n# Variance of \"income\"\nVar_income &lt;- var(random_vars[\"income\"])\n# Standard deviation of \"age\"\nStdev_age &lt;- sd(unlist(random_vars[\"age\"]))\n# Standard deviation of \"income\"\nStdev_income &lt;- sd(unlist(random_vars[\"income\"]))"
  },
  {
    "objectID": "content/01_journal/04_causality.html",
    "href": "content/01_journal/04_causality.html",
    "title": "Causality",
    "section": "",
    "text": "1 Spurious correlations\n\n# Importing data values\nmozzarella &lt;- c(9.3, 9.7, 9.7, 9.7, 9.9, 10.2, 10.5, 11, 10.6, 10.6)\ndoctorates &lt;- c(480, 501, 540, 552, 547, 622, 655, 701, 712, 708)\nyear &lt;- c(2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009)\ncor_data &lt;- data.frame(year, mozzarella, doctorates)\n\n# Plot data\nylim.prim &lt;- c(9, 12)\nylim.sec &lt;- c(400, 1000)\nb &lt;- diff(ylim.prim)/diff(ylim.sec)\na &lt;- (ylim.prim[1] - b*ylim.sec[1])\nbasic &lt;- ggplot(cor_data, aes(x=year, y=mozzarella, color = \"Mozzarella consumption\")) + geom_line() + geom_point() + geom_line(aes(y = a + doctorates*b, color = \"Test\"), color = \"cyan3\") + geom_point(aes(y = a + doctorates*b), color = \"cyan3\") + scale_y_continuous(\"Mozzarella cheese consumption\", sec.axis = sec_axis(~ (. - a)/b, name = \"Engineering doctorates\")) + scale_x_continuous(\"Year\", breaks = cor_data$year) + ggtitle(\"Per capita consumption of mozzarella cheese\\n correlates with\\nCivil engineering doctorates awarded\") + theme(plot.title = element_text(hjust=0.5, face=\"bold\"))\nbasic + labs(color=\"Legend\")\n\n\n\n\n\n\n\nCorrelation: 95.86% (r=0.958648)\n\n# Calculate correlation of mozzarella consumption and number of awarded doctorates\ncor(mozzarella, doctorates)\n\n#&gt; [1] 0.9586478"
  },
  {
    "objectID": "content/01_journal/09_iv.html",
    "href": "content/01_journal/09_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "# Randomized encouragement trial \nrand_enc_dag &lt;- dagify(\n  Y ~ X,\n  Z ~ Y,\n  coords = list(x = c(X = 1, Y = 3, Z = 5),\n                y = c(X = 1, Y = 1, Z = 1)),\n  labels = list(X = \"randomly encouraged\",\n                Y = \"use feature\",\n                Z = \"time spent on app\")\n)\n\n# Plot DAG\nggdag(rand_enc_dag) +\n  theme_dag() +\n  geom_dag_point(color = \"lightblue\") +\n  geom_dag_text(color = \"black\") +\n  geom_dag_edges(edge_color = \"black\") +\n  geom_dag_label_repel(aes(label = label))"
  },
  {
    "objectID": "content/01_journal/07_matching.html",
    "href": "content/01_journal/07_matching.html",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "online_store &lt;- dagify(\n  C ~ A,\n  C ~ B,\n  D ~ C,\n  D ~ A,\n  D ~ B,\n  E ~ A,\n  E ~ B,\n  E ~ D,\n  coords = list(x = c(A = 3, B = 4, C = 3, D = 1, E = 5),\n                y = c(A = 2, B = 2, C = 3, D = 1, E = 1)),\n  labels = list(A = \"age\",\n                B = \"sex\",\n                C = \"pre_avg_purch\",\n                D = \"card\",\n                E = \"avg_purch\")\n)\n\n# Plot DAG\nggdag(online_store) +\n  theme_dag() +\n  geom_dag_point(color = \"lightblue\") +\n  geom_dag_text(color = \"black\") +\n  geom_dag_edges(edge_color = \"black\") +\n  geom_dag_label_repel(aes(label = label))"
  },
  {
    "objectID": "content/01_journal/05_dag.html",
    "href": "content/01_journal/05_dag.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "# parking spots \nparking_spots &lt;- dagify(\n  Y ~ X,\n  coords = list(x = c(X = 1, Y = 3),\n                y = c(X = 1, Y = 1)),\n  labels = list(X = \"having parking spots\",\n                Y = \"sales\")\n)\n\n# Plot DAG\nggdag(parking_spots) +\n  theme_dag() +\n  geom_dag_point(color = \"lightblue\") +\n  geom_dag_text(color = \"black\") +\n  geom_dag_edges(edge_color = \"black\") +\n  geom_dag_label_repel(aes(label = label))"
  },
  {
    "objectID": "content/01_journal/06_rct.html",
    "href": "content/01_journal/06_rct.html",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "# Plot covariates balance\nm.out &lt;- MatchIt::matchit(chatbot ~ previous_visit + mobile_device, data = abtest_online)\nbal.plot(m.out, var.name = \"previous_visit\")\n\n\n\n\n\n\nbal.plot(m.out, var.name = \"mobile_device\")\n\n\n\n\n\n\n\nFrom the plots we observe that in general, the covariates are balanced across the groups. However, for previous_visit there are slightly fewer control units than treated units so that not all units get a match. Especially, the plot “Distributional Balance for previous_visit” shows that the from previous_visit = 7, there are no control units matched to the treated units."
  },
  {
    "objectID": "content/01_journal/08_did.html",
    "href": "content/01_journal/08_did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "# Treated hospitals before the treatment\nmean(dplyr::pull(dplyr::filter(hospdd, hospital &gt;= 1, hospital &lt;= 18, procedure==0), satis))\n\n#&gt; [1] 3.525383\n\n# Treated hospitals after the treatment\nmean(dplyr::pull(dplyr::filter(hospdd, hospital &gt;= 1, hospital &lt;= 18, procedure==1), satis))\n\n#&gt; [1] 4.363351\n\n# Control hospitals (without treatment)\nmean(dplyr::pull(dplyr::filter(hospdd, hospital &gt; 18, procedure==0), satis))\n\n#&gt; [1] 3.387499\n\n\n\nMean satisfaction for treated hospitals before the treatment: 3.525383\nMean satisfaction for treated hospitals after the treatment: 4.363351\nMean satisfaction for control hospitals (without treatment): 3.387499"
  },
  {
    "objectID": "content/01_journal/01_probability.html",
    "href": "content/01_journal/01_probability.html",
    "title": "Probability Theory",
    "section": "",
    "text": "\\(P(T \\cap S) = 0.3 \\cdot 0.2 = 0.06\\)\n\\(P(T \\cap \\overline{S}) = 0.7 \\cdot 0.6 = 0.42\\)\n\\(P(\\overline{T} \\cap S) = 0.3 \\cdot 0.8 = 0.24\\)\n\\(P(\\overline{T} \\cap \\overline{S}) = 0.7 \\cdot 0.4 = 0.28\\)\n\\(P(T \\cap S) + P(T \\cap \\overline{S}) + P(\\overline{T} \\cap S) + P(\\overline{T} \\cap \\overline{S}) = 0.06 + 0.42 + 0.24 + 0.28 = 1\\)"
  },
  {
    "objectID": "content/01_journal/01_probability.html#header-2",
    "href": "content/01_journal/01_probability.html#header-2",
    "title": "Probability Theory",
    "section": "2.1 Header 2",
    "text": "2.1 Header 2\n\nHeader 3\n\nHeader 4\n\nHeader 5\n\nHeader 6"
  },
  {
    "objectID": "content/01_journal/03_regression.html",
    "href": "content/01_journal/03_regression.html",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "The data has 181 rows and 22 columns.\n\ndim(car_prices)\n\n#&gt; [1] 181  22"
  },
  {
    "objectID": "content/01_journal/10_rdd.html",
    "href": "content/01_journal/10_rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/01_probability.html#assignment-i",
    "href": "content/01_journal/01_probability.html#assignment-i",
    "title": "Probability Theory",
    "section": "",
    "text": "\\(P(T \\cap S) = 0.3 \\cdot 0.2 = 0.06\\)\n\\(P(T \\cap \\overline{S}) = 0.7 \\cdot 0.6 = 0.42\\)\n\\(P(\\overline{T} \\cap S) = 0.3 \\cdot 0.8 = 0.24\\)\n\\(P(\\overline{T} \\cap \\overline{S}) = 0.7 \\cdot 0.4 = 0.28\\)\n\\(P(T \\cap S) + P(T \\cap \\overline{S}) + P(\\overline{T} \\cap S) + P(\\overline{T} \\cap \\overline{S}) = 0.06 + 0.42 + 0.24 + 0.28 = 1\\)"
  },
  {
    "objectID": "content/01_journal/01_probability.html#assignment-ii",
    "href": "content/01_journal/01_probability.html#assignment-ii",
    "title": "Probability Theory",
    "section": "2.1 Assignment II",
    "text": "2.1 Assignment II\n\nThe percentage of customers using all three devices is \\(0.5\\%\\).\nThe percentage of customers using at least two devices is \\(19.9\\%\\).\nThe percentage of customers using only one device is \\(80.1\\%\\)."
  },
  {
    "objectID": "content/01_journal/01_probability.html#assignment-iii",
    "href": "content/01_journal/01_probability.html#assignment-iii",
    "title": "Probability Theory",
    "section": "3.1 Assignment III",
    "text": "3.1 Assignment III\n\n\\(P(\\overline{A}|B) = \\frac{0.01*(1-0.04)}{0.97*0.04+0.01*(1-0.04)} = 0.198\\)\n\\(P(A|B) = \\frac{0.97*0.04}{0.97*0.04+0.01*(1-0.04)} = 0.802\\)"
  },
  {
    "objectID": "content/01_journal/01_probability_org.html",
    "href": "content/01_journal/01_probability_org.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh.\nThis is a .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "content/01_journal/01_probability_org.html#header-2",
    "href": "content/01_journal/01_probability_org.html#header-2",
    "title": "Probability Theory",
    "section": "2.1 Header 2",
    "text": "2.1 Header 2\n\nHeader 3\n\nHeader 4\n\nHeader 5\n\nHeader 6"
  },
  {
    "objectID": "content/01_journal/07_matching.html#dag-of-online-store",
    "href": "content/01_journal/07_matching.html#dag-of-online-store",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "online_store &lt;- dagify(\n  C ~ A,\n  C ~ B,\n  D ~ C,\n  D ~ A,\n  D ~ B,\n  E ~ A,\n  E ~ B,\n  E ~ D,\n  coords = list(x = c(A = 3, B = 4, C = 3, D = 1, E = 5),\n                y = c(A = 2, B = 2, C = 3, D = 1, E = 1)),\n  labels = list(A = \"age\",\n                B = \"sex\",\n                C = \"pre_avg_purch\",\n                D = \"card\",\n                E = \"avg_purch\")\n)\n\n# Plot DAG\nggdag(online_store) +\n  theme_dag() +\n  geom_dag_point(color = \"lightblue\") +\n  geom_dag_text(color = \"black\") +\n  geom_dag_edges(edge_color = \"black\") +\n  geom_dag_label_repel(aes(label = label))"
  },
  {
    "objectID": "content/01_journal/07_matching.html#naive-estimate-of-the-average-treatment-effect",
    "href": "content/01_journal/07_matching.html#naive-estimate-of-the-average-treatment-effect",
    "title": "Matching and Subclassification",
    "section": "\n2 Naive estimate of the average treatment effect",
    "text": "2 Naive estimate of the average treatment effect\nWe estimate the average treatment effect by taking the difference between expected values of sales in treatment group (plus membership) and control group (non plus membership).\n\\(ATE = E[avg\\_purch|card = 1] - E[avg\\_purch|card = 1] = 25.2195\\)\n\n# Expected values in treatment group\nE_avgpurch_card_1 = mean(unlist(membership[membership$card == 1,\"avg_purch\"]))\n# Expected values in control group\nE_avgpurch_card_0 = mean(unlist(membership[membership$card == 0,\"avg_purch\"]))\n# Naive estimate of ATE\nE_avgpurch_card_1 - E_avgpurch_card_0\n\n#&gt; [1] 25.2195\n\n\nA naive estimate obtained by regressing avg_purch on card.\n\nmodel_naive &lt;- lm(avg_purch ~ card, data = membership)\nsummary(model_naive)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = membership)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -101.515  -20.684   -0.199   20.424  120.166 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  65.9397     0.3965  166.29   &lt;2e-16 ***\n#&gt; card         25.2195     0.6095   41.38   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.11 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.1462, Adjusted R-squared:  0.1461 \n#&gt; F-statistic:  1712 on 1 and 9998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/07_matching.html#matching-methods-for-more-precise-estimates",
    "href": "content/01_journal/07_matching.html#matching-methods-for-more-precise-estimates",
    "title": "Matching and Subclassification",
    "section": "\n3 Matching methods for more precise estimates",
    "text": "3 Matching methods for more precise estimates\n1. (Coarsened) Exact Matching\n\n# Without specifying coarsening\n# (1) Matching\ncem &lt;- matchit(card ~ age + sex + pre_avg_purch,\n               data = membership, \n               method = 'cem', \n               estimand = 'ATE')\n# Use matched data\ndf_cem &lt;- match.data(cem)\n\n# (2) Estimation\nmodel_cem &lt;- lm(avg_purch ~ card, data = df_cem, weights = weights)\nsummary(model_cem)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_cem, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -159.349  -20.459   -0.151   19.863  161.528 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  69.9896     0.3984  175.66   &lt;2e-16 ***\n#&gt; card         15.2043     0.6137   24.77   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.12 on 9878 degrees of freedom\n#&gt; Multiple R-squared:  0.0585, Adjusted R-squared:  0.0584 \n#&gt; F-statistic: 613.7 on 1 and 9878 DF,  p-value: &lt; 2.2e-16\n\n\n2. Nearest-Neighbor Matching\n\n# (1) Matching\n# replace: one-to-one or one-to-many matching\nnn &lt;- matchit(card ~ age + sex + pre_avg_purch,\n              data = membership,\n              method = \"nearest\", \n              distance = \"mahalanobis\",\n              replace = T)\n\n# Use matched data\ndf_nn &lt;- match.data(nn)\n\n# (2) Estimation\nmodel_nn &lt;- lm(avg_purch ~ card, data = df_nn, weights = weights)\nsummary(model_nn)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_nn, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -132.730  -21.288   -1.675   18.318  146.631 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  76.5634     0.5881  130.19   &lt;2e-16 ***\n#&gt; card         14.5957     0.7514   19.42   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.43 on 6907 degrees of freedom\n#&gt; Multiple R-squared:  0.05179,    Adjusted R-squared:  0.05166 \n#&gt; F-statistic: 377.3 on 1 and 6907 DF,  p-value: &lt; 2.2e-16\n\n\n3. Inverse Probability Weighting\n\n# (1) Propensity scores\nmodel_prop &lt;- glm(card ~ age + sex + pre_avg_purch,\n                  data = membership,\n                  family = binomial(link = \"logit\"))\nsummary(model_prop)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = card ~ age + sex + pre_avg_purch, family = binomial(link = \"logit\"), \n#&gt;     data = membership)\n#&gt; \n#&gt; Coefficients:\n#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)   -1.4298676  0.0752043 -19.013   &lt;2e-16 ***\n#&gt; age            0.0011486  0.0017761   0.647    0.518    \n#&gt; sex            0.0359388  0.0412622   0.871    0.384    \n#&gt; pre_avg_purch  0.0148262  0.0009264  16.003   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 13626  on 9999  degrees of freedom\n#&gt; Residual deviance: 13249  on 9996  degrees of freedom\n#&gt; AIC: 13257\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n# Add propensities to table\ndf_aug &lt;- membership %&gt;% mutate(propensity = predict(model_prop, type = \"response\"))\n\n# Extend data by IPW scores\ndf_ipw &lt;- df_aug %&gt;% mutate(\n  ipw = (card/propensity) + ((1-card) / (1-propensity)))\n\n# (2) Estimation\nmodel_ipw &lt;- lm(avg_purch ~ card,\n                data = df_ipw, \n                weights = ipw)\nsummary(model_ipw)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_ipw, weights = ipw)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -205.353  -28.995   -0.275   28.787  214.307 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  70.2628     0.4320  162.66   &lt;2e-16 ***\n#&gt; card         14.9573     0.6109   24.48   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 43.19 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.05657,    Adjusted R-squared:  0.05647 \n#&gt; F-statistic: 599.5 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n# Plot histogram of estimated propensities\nggplot(df_aug, aes(x = propensity)) +\n  geom_histogram(alpha = .8, color = \"white\", bins=30)\n\n\n\n\n\n\n\n4. Comparison of methods\n\n# Summary of naive and matching methods\nmodelsummary::modelsummary(list(\"Naive\" = model_naive,\n                                \"CEM\"   = model_cem,\n                                \"NN\"    = model_nn,\n                                \"IPW\"   = model_ipw))\n\n\n\n\nNaive\nCEM\nNN\n IPW\n\n\n\n(Intercept)\n65.940\n69.990\n76.563\n70.263\n\n\n\n(0.397)\n(0.398)\n(0.588)\n(0.432)\n\n\ncard\n25.220\n15.204\n14.596\n14.957\n\n\n\n(0.610)\n(0.614)\n(0.751)\n(0.611)\n\n\nNum.Obs.\n10000\n9880\n6909\n10000\n\n\nR2\n0.146\n0.058\n0.052\n0.057\n\n\nR2 Adj.\n0.146\n0.058\n0.052\n0.056\n\n\nAIC\n96483.2\n95595.0\n67134.2\n97072.1\n\n\nBIC\n96504.8\n95616.6\n67154.7\n97093.7\n\n\nLog.Lik.\n−48238.590\n−47794.497\n−33564.098\n−48533.031\n\n\nRMSE\n30.11\n30.08\n30.17\n30.54"
  },
  {
    "objectID": "content/01_journal/02_statistics.html#expected-value-variance-standard-deviation",
    "href": "content/01_journal/02_statistics.html#expected-value-variance-standard-deviation",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Expected value of\n\nage: \\(E[age] = 33.471\\)\n\nincome: \\(E[income] = 3510.731\\)\n\n\n\nVariance of\n\nage: \\(Var[age] = 340.6078\\)\n\nincome: \\(Var[income] = 8625646\\)\n\n\n\nStandard deviation of\n\nage: \\(\\sigma = 18.45556\\)\n\nincome: \\(\\sigma = 2936.945\\)\n\n\n\n\n\n# Expected value of \"age\"\nE_age &lt;- mean(unlist(random_vars[\"age\"]))\n# Expected value of \"income\"\nE_income &lt;- mean(unlist(random_vars[\"income\"]))\n# Variance of \"age\"\nVar_age &lt;- var(random_vars[\"age\"])\n# Variance of \"income\"\nVar_income &lt;- var(random_vars[\"income\"])\n# Standard deviation of \"age\"\nStdev_age &lt;- sd(unlist(random_vars[\"age\"]))\n# Standard deviation of \"income\"\nStdev_income &lt;- sd(unlist(random_vars[\"income\"]))"
  },
  {
    "objectID": "content/01_journal/02_statistics.html#comparing-standard-deviations",
    "href": "content/01_journal/02_statistics.html#comparing-standard-deviations",
    "title": "Statistical Concepts",
    "section": "\n2 Comparing standard deviations",
    "text": "2 Comparing standard deviations\nComparing standard deviations between age and income may not provide meaningful insights. The two variables are different types of data, thus they have different ranges. Age has a range from 0 to 99, but income has a range from 0 to 10485. Furthermore, both variables have different measure units. Age is measured in years and income is measured in a type of currency. This makes their standard deviations also to have different unit, thus they are not comparable to each other."
  },
  {
    "objectID": "content/01_journal/02_statistics.html#computing-covariance-and-correlation",
    "href": "content/01_journal/02_statistics.html#computing-covariance-and-correlation",
    "title": "Statistical Concepts",
    "section": "\n3 Computing covariance and correlation",
    "text": "3 Computing covariance and correlation\n\nCovariance: \\(Cov(age,income) = 29700.15\\)\n\nCorrelation: \\(Corr(age,income) = 0.5479432\\)\n\n\nBoth the covariance and correlation indicate a positive linear relationship between age and income. In other words, on average, when age increases, income increases proportionally.\n\n# Covariance\nCovariance &lt;- cov(random_vars[\"age\"], random_vars[\"income\"])\n# Correlation\nCorrelation &lt;- cor(random_vars[\"age\"], random_vars[\"income\"])"
  },
  {
    "objectID": "content/01_journal/02_statistics.html#interpretation-of-covariance-and-correlation",
    "href": "content/01_journal/02_statistics.html#interpretation-of-covariance-and-correlation",
    "title": "Statistical Concepts",
    "section": "\n4 Interpretation of covariance and correlation",
    "text": "4 Interpretation of covariance and correlation\nCorrelation is easier to interpret than covariance. Since correlation is a standardized measure ranges from -1 to 1, one can find out the strength of the relationship between two variables in addition to its direction. The closer the absolute value of correlation is to 1, the stronger the relationship. In contrast, covariance is not a standardized measure, since its values don’t have a specific range. Covariance only provides information to the direction of the relationship."
  },
  {
    "objectID": "content/01_journal/02_statistics.html#conditional-expected-value",
    "href": "content/01_journal/02_statistics.html#conditional-expected-value",
    "title": "Statistical Concepts",
    "section": "\n5 Conditional expected value",
    "text": "5 Conditional expected value\n\n\\(E[income|age&lt;=18] = 389.6074\\)\n\\(E[income|age \\in [18,65)] = 4685.734\\)\n\\(E[income|age&gt;=65] = 1777.237\\)\n\n\n# Expected value of income with age under or equal 18\nE_1 &lt;- mean(unlist(random_vars[random_vars$age &lt;= 18,\"income\"]))\n# Expected value of income with age above or equal 18 AND under 65\nE_2 &lt;- mean(unlist(random_vars[random_vars$age &gt;= 18 & random_vars$age &lt; 65,\"income\"]))\n# Expected value of income with age above or equal 65\nE_3 &lt;- mean(unlist(random_vars[random_vars$age &gt;= 65,\"income\"]))"
  },
  {
    "objectID": "content/01_journal/03_regression.html#dimensions-of-data",
    "href": "content/01_journal/03_regression.html#dimensions-of-data",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "The data has 181 rows and 22 columns.\n\ndim(car_prices)\n\n#&gt; [1] 181  22"
  },
  {
    "objectID": "content/01_journal/03_regression.html#data-types",
    "href": "content/01_journal/03_regression.html#data-types",
    "title": "Regression and Statistical Inference",
    "section": "\n2 Data types",
    "text": "2 Data types\nThere are three data types in the data: integer, numeric, character. Numbers can be represented as different data types. Whole numbers have integer data types, while decimal numbers are represented as numeric data types. Strings are represented as character data types and can store not only letters,symbols or sentences but also numbers. Strings must be enclosed in single or double quotes."
  },
  {
    "objectID": "content/01_journal/03_regression.html#linear-regression",
    "href": "content/01_journal/03_regression.html#linear-regression",
    "title": "Regression and Statistical Inference",
    "section": "\n3 Linear regression",
    "text": "3 Linear regression\nThe factors that are relevant for the pricing of a car are: car width, engine size, stroke, peak rpm. These four factors are statistically significant at different levels of \\(\\alpha\\) (\\(\\alpha = 0\\) and \\(\\alpha = .001\\)), meaning there is a correlation between the independent variables (carwidth, enginesize, stroke, peakrpm) and the dependent variable price.\n\n# Linear regression of price on all regressors\nlm_car_prices &lt;- lm(price ~ ., data = car_prices)\nlm_summary &lt;- summary(lm_car_prices)"
  },
  {
    "objectID": "content/01_journal/03_regression.html#analyzing-one-regressor",
    "href": "content/01_journal/03_regression.html#analyzing-one-regressor",
    "title": "Regression and Statistical Inference",
    "section": "\n4 Analyzing one regressor",
    "text": "4 Analyzing one regressor\nChosen regressor: peakrpm 1. The regressor has integer data type. The values of peakrpm are positive whole numbers without a decimal point. 2. An increase of one unit in peakrpm is related to an \\(2.416\\) increase in price. 3. The above mentioned effect is statistically significant, since the respective p-value is lower than the significance level \\(\\alpha = 0\\)."
  },
  {
    "objectID": "content/01_journal/03_regression.html#regression-with-seat_heating",
    "href": "content/01_journal/03_regression.html#regression-with-seat_heating",
    "title": "Regression and Statistical Inference",
    "section": "\n5 Regression with seat_heating\n",
    "text": "5 Regression with seat_heating\n\nI get NA as coefficient for the new variable seat_heating. This is due to the fact that some of the assumptions for a linear regression to deliver valid results are not fulfilled since the variable only takes on one unique value. The linearity, homoscedasticity and normality conditions are not fulfilled, since the variable doesn’t provide any variability for the regression model. Due to this lack of information and variability, R cannot find any relationship between independent and dependent variable when performing linear regression.\n\n# Add variable 'seat_heating' to data\ncar_prices &lt;- car_prices %&gt;% mutate(seat_heating = TRUE)\n\n# Linear regression of price on all regressors\nlm_car_prices &lt;- lm(price ~ ., data = car_prices)\nlm_summary &lt;- summary(lm_car_prices)"
  },
  {
    "objectID": "content/01_journal/05_dag.html#dag-of-the-parking-spots-example",
    "href": "content/01_journal/05_dag.html#dag-of-the-parking-spots-example",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "# parking spots \nparking_spots &lt;- dagify(\n  Y ~ X,\n  coords = list(x = c(X = 1, Y = 3),\n                y = c(X = 1, Y = 1)),\n  labels = list(X = \"having parking spots\",\n                Y = \"sales\")\n)\n\n# Plot DAG\nggdag(parking_spots) +\n  theme_dag() +\n  geom_dag_point(color = \"lightblue\") +\n  geom_dag_text(color = \"black\") +\n  geom_dag_edges(edge_color = \"black\") +\n  geom_dag_label_repel(aes(label = label))"
  },
  {
    "objectID": "content/01_journal/05_dag.html#customer-satisfaction",
    "href": "content/01_journal/05_dag.html#customer-satisfaction",
    "title": "Directed Acyclic Graphs",
    "section": "\n2 Customer satisfaction",
    "text": "2 Customer satisfaction\n\nRegress satisfaction on follow_ups\n\n\n\nlm_sat_fol &lt;- lm(satisfaction ~ follow_ups, data = customer_sat)\nsummary(lm_sat_fol)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups, data = customer_sat)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -12.412  -5.257   1.733   4.506  12.588 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  78.8860     4.2717  18.467 1.04e-10 ***\n#&gt; follow_ups   -3.3093     0.6618  -5.001 0.000243 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 7.923 on 13 degrees of freedom\n#&gt; Multiple R-squared:  0.658,  Adjusted R-squared:  0.6316 \n#&gt; F-statistic: 25.01 on 1 and 13 DF,  p-value: 0.0002427\n\n\n\nRegress satisfaction on follow_ups and account for subscription\n\n\n\nlm_sat_fol_subs &lt;- lm(satisfaction ~ follow_ups + subscription, data = customer_sat)\nsummary(lm_sat_fol_subs)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups + subscription, data = customer_sat)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.3222 -2.1972  0.3167  2.2667  3.9944 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)           26.7667     6.6804   4.007  0.00206 ** \n#&gt; follow_ups             2.1944     0.7795   2.815  0.01682 *  \n#&gt; subscriptionPremium   44.7222     5.6213   7.956 6.88e-06 ***\n#&gt; subscriptionPremium+  18.0722     2.1659   8.344 4.37e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.958 on 11 degrees of freedom\n#&gt; Multiple R-squared:  0.9597, Adjusted R-squared:  0.9487 \n#&gt; F-statistic: 87.21 on 3 and 11 DF,  p-value: 5.956e-08"
  },
  {
    "objectID": "content/01_journal/05_dag.html#comparison-of-coefficients",
    "href": "content/01_journal/05_dag.html#comparison-of-coefficients",
    "title": "Directed Acyclic Graphs",
    "section": "\n3 Comparison of coefficients",
    "text": "3 Comparison of coefficients\nThe coefficient in the first case (Regress satisfaction on follow_ups) is negative, meaning an increase in follow-up calls is related to a decrease in satisfaction. However, when we account for subscription in the linear regression, the coefficient indicates a positive linear relationship between follow-up calls and satisfaction. This is due to the fact that the number of follow-up calls depends on the subscription, which also affects the satisfaction degree. Thus, subscription is a cofounder and we would result in an incorrect result if we do not account for subscription when performing regression. The DAG for the example looks like this\n\n\n\n\n\n\n\n\nLooking at the DAG above, we see that there is an indirect effect induced by subscription on the direct causal effect of follow_ups on satisfaction. The indirect effect is a spurious correlation which is irrelevant for our research, but can falsify our results if we do not conditioned on it. By accounting for subscription when regressing satisfaction on follow_ups, we condition on subscription and keep it at a fixed value. Thus, the values of satisfaction are purely dependent on the values of follow_ups, and we are able to retrieve the causal effect from follow_ups to satisfaction."
  },
  {
    "objectID": "content/01_journal/05_dag.html#plot",
    "href": "content/01_journal/05_dag.html#plot",
    "title": "Directed Acyclic Graphs",
    "section": "\n4 Plot",
    "text": "4 Plot\n\n# Plot of first regression\nggplot(customer_sat, aes(x = follow_ups, y = satisfaction,)) +\n  geom_point(color = \"cornflowerblue\") +\n  stat_smooth(method = \"lm\", se = F, color = \"coral2\", formula = y ~ x)\n\n\n\n\n\n\n\n\n# Plot of second regression\nggplot(customer_sat, aes(x = follow_ups, y = satisfaction,\n                            color = subscription, \n                            alpha = subscription)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = F, formula = y ~ x) +\n  scale_color_manual(values = c(\"Premium\"=\"darkolivegreen3\",\n                                \"Premium+\"=\"cornflowerblue\",\n                                \"Elite\"=\"coral2\")) +\n  scale_alpha_manual(values = c(1,1, 1)) +\n  theme(legend.position = \"right\")"
  },
  {
    "objectID": "content/01_journal/06_rct.html#covariates-balance-plots",
    "href": "content/01_journal/06_rct.html#covariates-balance-plots",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "# Plot covariates balance\nm.out &lt;- MatchIt::matchit(chatbot ~ previous_visit + mobile_device, data = abtest_online)\nbal.plot(m.out, var.name = \"previous_visit\")\n\n\n\n\n\n\nbal.plot(m.out, var.name = \"mobile_device\")\n\n\n\n\n\n\n\nFrom the plots we observe that in general, the covariates are balanced across the groups. However, for previous_visit there are slightly fewer control units than treated units so that not all units get a match. Especially, the plot “Distributional Balance for previous_visit” shows that the from previous_visit = 7, there are no control units matched to the treated units."
  },
  {
    "objectID": "content/01_journal/06_rct.html#regress-chatbot-on-purchase_amount",
    "href": "content/01_journal/06_rct.html#regress-chatbot-on-purchase_amount",
    "title": "Randomized Controlled Trials",
    "section": "\n2 Regress chatbot on purchase_amount\n",
    "text": "2 Regress chatbot on purchase_amount\n\n\nlm_sales &lt;- lm(purchase_amount ~ chatbot, data = abtest_online)\nsummary(lm_sales)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot, data = abtest_online)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -16.702 -14.478  -9.626  13.922  64.648 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  16.7017     0.8374  19.944  &lt; 2e-16 ***\n#&gt; chatbotTRUE  -7.0756     1.1796  -5.998 2.79e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.65 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.0348, Adjusted R-squared:  0.03383 \n#&gt; F-statistic: 35.98 on 1 and 998 DF,  p-value: 2.787e-09\n\n# Plot result\njtools::effect_plot(lm_sales, pred = chatbot)\n\n\n\n\n\n\n\nThe coefficient of the regression is \\(-7.0756\\) for chatbot = TRUE. This means that the sales dropped for the group of customers that were directed to a chatbot. The result also reflects in the plot."
  },
  {
    "objectID": "content/01_journal/06_rct.html#subgroup-specific-effects",
    "href": "content/01_journal/06_rct.html#subgroup-specific-effects",
    "title": "Randomized Controlled Trials",
    "section": "\n3 Subgroup-specific effects",
    "text": "3 Subgroup-specific effects\nThe chosen subgroup is mobile users.\n\n# Linear regression on subgroup\nlm_CATE &lt;- lm(purchase_amount ~ chatbot*mobile_device, data = abtest_online)\nsummary(lm_CATE)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot * mobile_device, data = abtest_online)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -16.98 -14.54  -9.95  14.13  65.24 \n#&gt; \n#&gt; Coefficients:\n#&gt;                               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                    16.9797     1.0152  16.725   &lt;2e-16 ***\n#&gt; chatbotTRUE                    -7.0301     1.4284  -4.922    1e-06 ***\n#&gt; mobile_deviceTRUE              -0.8727     1.7987  -0.485    0.628    \n#&gt; chatbotTRUE:mobile_deviceTRUE  -0.1526     2.5369  -0.060    0.952    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.66 on 996 degrees of freedom\n#&gt; Multiple R-squared:  0.03534,    Adjusted R-squared:  0.03244 \n#&gt; F-statistic: 12.16 on 3 and 996 DF,  p-value: 8.034e-08"
  },
  {
    "objectID": "content/01_journal/06_rct.html#logistic-regression-of-purchase",
    "href": "content/01_journal/06_rct.html#logistic-regression-of-purchase",
    "title": "Randomized Controlled Trials",
    "section": "\n4 Logistic regression of purchase\n",
    "text": "4 Logistic regression of purchase\n\n\nlogreg &lt;- glm(purchase ~ chatbot, data = abtest_online, family=binomial)\nsummary(logreg)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = purchase ~ chatbot, family = binomial, data = abtest_online)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept) -0.01613    0.08981  -0.180    0.857    \n#&gt; chatbotTRUE -0.98939    0.13484  -7.337 2.18e-13 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 1329.1  on 999  degrees of freedom\n#&gt; Residual deviance: 1273.3  on 998  degrees of freedom\n#&gt; AIC: 1277.3\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n# Calculate odds ratio\nodds_ratio &lt;- exp(-0.98939)\n# Convert odds ratio to percentage\npercentage &lt;- odds_ratio/(1+odds_ratio)\n\nThe coefficient for chatbot = TRUE is \\(-0.98939\\) which corresponds to the log of odds ratio between the group of customers that used the chatbot and that interacted with human customer service. The log odds is the logarithm of the odds, which is the ratio of something happening to something not happening. The odds ratio equals \\(0.37\\) is less than \\(1\\), which means that it is unlikely that a customer using a chatbot will purchase something. In particular, the odds of purchasing something for customers who didn’t use a chatbot are about \\(27\\%\\) higher than the odds for customers who used a chatbot."
  },
  {
    "objectID": "content/01_journal/08_did.html#mean-satisfaction-for-treated-and-control-hospitals",
    "href": "content/01_journal/08_did.html#mean-satisfaction-for-treated-and-control-hospitals",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "# Treated hospitals before the treatment\nmean(dplyr::pull(dplyr::filter(hospdd, hospital &gt;= 1, hospital &lt;= 18, procedure==0), satis))\n\n#&gt; [1] 3.525383\n\n# Treated hospitals after the treatment\nmean(dplyr::pull(dplyr::filter(hospdd, hospital &gt;= 1, hospital &lt;= 18, procedure==1), satis))\n\n#&gt; [1] 4.363351\n\n# Control hospitals (without treatment)\nmean(dplyr::pull(dplyr::filter(hospdd, hospital &gt; 18, procedure==0), satis))\n\n#&gt; [1] 3.387499\n\n\n\nMean satisfaction for treated hospitals before the treatment: 3.525383\nMean satisfaction for treated hospitals after the treatment: 4.363351\nMean satisfaction for control hospitals (without treatment): 3.387499"
  },
  {
    "objectID": "content/01_journal/08_did.html#linear-regression",
    "href": "content/01_journal/08_did.html#linear-regression",
    "title": "Difference-in-Differences",
    "section": "\n2 Linear regression",
    "text": "2 Linear regression\n\n# Treated hospitals before introduction\ndf_A1_lm &lt;- (dplyr::filter(hospdd, hospital &gt;= 1, hospital &lt;= 18, procedure == 0))\n# Treated hospitals after introduction\ndf_A2_lm &lt;- (dplyr::filter(hospdd, hospital &gt;= 1, hospital &lt;= 18, procedure == 1))\n# Control hospitals\ndf_B_lm &lt;- (dplyr::filter(hospdd, hospital &gt; 18))\n\n\nLinear regression on treated hospitals before introduction\n\n\nsummary(lm(satis ~ as.factor(hospital) + as.factor(month), data = df_A1_lm))\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ as.factor(hospital) + as.factor(month), \n#&gt;     data = df_A1_lm)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.1353 -0.4757 -0.0048  0.4740  3.8750 \n#&gt; \n#&gt; Coefficients:\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)            3.228870   0.080191  40.265  &lt; 2e-16 ***\n#&gt; as.factor(hospital)2   0.288897   0.112740   2.562 0.010488 *  \n#&gt; as.factor(hospital)3   0.452987   0.115801   3.912 9.57e-05 ***\n#&gt; as.factor(hospital)4   0.311332   0.107923   2.885 0.003973 ** \n#&gt; as.factor(hospital)5  -0.274068   0.107923  -2.539 0.011201 *  \n#&gt; as.factor(hospital)6   0.368605   0.107923   3.415 0.000654 ***\n#&gt; as.factor(hospital)7   1.335041   0.104296  12.801  &lt; 2e-16 ***\n#&gt; as.factor(hospital)8   0.047233   0.111393   0.424 0.671609    \n#&gt; as.factor(hospital)9  -1.551302   0.114204 -13.584  &lt; 2e-16 ***\n#&gt; as.factor(hospital)10  1.572914   0.112740  13.952  &lt; 2e-16 ***\n#&gt; as.factor(hospital)11  0.162248   0.111393   1.457 0.145453    \n#&gt; as.factor(hospital)12 -0.073736   0.114204  -0.646 0.518602    \n#&gt; as.factor(hospital)13  0.462809   0.110148   4.202 2.80e-05 ***\n#&gt; as.factor(hospital)14  0.141160   0.115801   1.219 0.223038    \n#&gt; as.factor(hospital)15 -0.300102   0.115801  -2.592 0.009647 ** \n#&gt; as.factor(hospital)16  1.376744   0.112740  12.212  &lt; 2e-16 ***\n#&gt; as.factor(hospital)17  0.368000   0.117549   3.131 0.001778 ** \n#&gt; as.factor(hospital)18  0.222772   0.136932   1.627 0.103972    \n#&gt; as.factor(month)2     -0.013209   0.046752  -0.283 0.777577    \n#&gt; as.factor(month)3      0.005673   0.046752   0.121 0.903443    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.7471 on 1512 degrees of freedom\n#&gt; Multiple R-squared:  0.4626, Adjusted R-squared:  0.4558 \n#&gt; F-statistic: 68.49 on 19 and 1512 DF,  p-value: &lt; 2.2e-16\n\n\n\nLinear regression on treated hospitals after introduction\n\n\nsummary(lm(satis ~ as.factor(hospital) + as.factor(month), data = df_A2_lm))\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ as.factor(hospital) + as.factor(month), \n#&gt;     data = df_A2_lm)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -2.9249 -0.5702  0.0051  0.5312  4.3010 \n#&gt; \n#&gt; Coefficients:\n#&gt;                       Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)            3.93810    0.09656  40.785  &lt; 2e-16 ***\n#&gt; as.factor(hospital)2   0.52824    0.12866   4.106 4.24e-05 ***\n#&gt; as.factor(hospital)3   0.61426    0.13215   4.648 3.64e-06 ***\n#&gt; as.factor(hospital)4   0.14369    0.12316   1.167 0.243517    \n#&gt; as.factor(hospital)5  -0.01664    0.12316  -0.135 0.892553    \n#&gt; as.factor(hospital)6   0.52712    0.12316   4.280 1.99e-05 ***\n#&gt; as.factor(hospital)7   1.47379    0.11902  12.383  &lt; 2e-16 ***\n#&gt; as.factor(hospital)8   0.09652    0.12712   0.759 0.447804    \n#&gt; as.factor(hospital)9  -1.48573    0.13033 -11.400  &lt; 2e-16 ***\n#&gt; as.factor(hospital)10  1.79278    0.12866  13.935  &lt; 2e-16 ***\n#&gt; as.factor(hospital)11  0.27968    0.12712   2.200 0.027945 *  \n#&gt; as.factor(hospital)12 -0.11687    0.13033  -0.897 0.369994    \n#&gt; as.factor(hospital)13  0.52838    0.12570   4.204 2.78e-05 ***\n#&gt; as.factor(hospital)14  0.32493    0.13215   2.459 0.014052 *  \n#&gt; as.factor(hospital)15  0.01112    0.13215   0.084 0.932978    \n#&gt; as.factor(hospital)16  1.45179    0.12866  11.284  &lt; 2e-16 ***\n#&gt; as.factor(hospital)17  0.47909    0.13414   3.571 0.000366 ***\n#&gt; as.factor(hospital)18  0.08378    0.15626   0.536 0.591934    \n#&gt; as.factor(month)5      0.04133    0.06161   0.671 0.502359    \n#&gt; as.factor(month)6      0.01224    0.06161   0.199 0.842484    \n#&gt; as.factor(month)7      0.03594    0.06161   0.583 0.559682    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.8525 on 1511 degrees of freedom\n#&gt; Multiple R-squared:  0.4118, Adjusted R-squared:  0.404 \n#&gt; F-statistic: 52.88 on 20 and 1511 DF,  p-value: &lt; 2.2e-16\n\n\n\nLinear regression on control hospitals\n\n\nsummary(lm(satis ~ as.factor(hospital) + as.factor(month), data = df_B_lm))\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ as.factor(hospital) + as.factor(month), \n#&gt;     data = df_B_lm)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -2.7339 -0.4212  0.0041  0.4310  2.9810 \n#&gt; \n#&gt; Coefficients:\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)            2.422814   0.056570  42.828  &lt; 2e-16 ***\n#&gt; as.factor(hospital)20  0.792689   0.074240  10.677  &lt; 2e-16 ***\n#&gt; as.factor(hospital)21  1.939639   0.078279  24.778  &lt; 2e-16 ***\n#&gt; as.factor(hospital)22  1.544617   0.077123  20.028  &lt; 2e-16 ***\n#&gt; as.factor(hospital)23  1.447022   0.076074  19.021  &lt; 2e-16 ***\n#&gt; as.factor(hospital)24  0.437176   0.080987   5.398 7.10e-08 ***\n#&gt; as.factor(hospital)25  1.391775   0.086459  16.098  &lt; 2e-16 ***\n#&gt; as.factor(hospital)26  0.959549   0.074240  12.925  &lt; 2e-16 ***\n#&gt; as.factor(hospital)27  0.346647   0.072003   4.814 1.53e-06 ***\n#&gt; as.factor(hospital)28  1.457297   0.078279  18.617  &lt; 2e-16 ***\n#&gt; as.factor(hospital)29  0.993853   0.075117  13.231  &lt; 2e-16 ***\n#&gt; as.factor(hospital)30  0.577380   0.088835   6.499 8.99e-11 ***\n#&gt; as.factor(hospital)31  1.257387   0.074240  16.937  &lt; 2e-16 ***\n#&gt; as.factor(hospital)32  0.421956   0.075117   5.617 2.06e-08 ***\n#&gt; as.factor(hospital)33  0.291327   0.074240   3.924 8.84e-05 ***\n#&gt; as.factor(hospital)34  0.744889   0.070213  10.609  &lt; 2e-16 ***\n#&gt; as.factor(hospital)35  1.099413   0.072003  15.269  &lt; 2e-16 ***\n#&gt; as.factor(hospital)36  2.883444   0.072691  39.667  &lt; 2e-16 ***\n#&gt; as.factor(hospital)37  0.885705   0.086459  10.244  &lt; 2e-16 ***\n#&gt; as.factor(hospital)38  0.658496   0.073435   8.967  &lt; 2e-16 ***\n#&gt; as.factor(hospital)39  0.721805   0.077123   9.359  &lt; 2e-16 ***\n#&gt; as.factor(hospital)40  1.866835   0.073435  25.422  &lt; 2e-16 ***\n#&gt; as.factor(hospital)41  0.595567   0.072003   8.271  &lt; 2e-16 ***\n#&gt; as.factor(hospital)42  1.626439   0.079560  20.443  &lt; 2e-16 ***\n#&gt; as.factor(hospital)43 -0.027131   0.076074  -0.357    0.721    \n#&gt; as.factor(hospital)44  0.779714   0.084396   9.239  &lt; 2e-16 ***\n#&gt; as.factor(hospital)45  0.531552   0.072003   7.382 1.86e-13 ***\n#&gt; as.factor(hospital)46  0.823793   0.077123  10.682  &lt; 2e-16 ***\n#&gt; as.factor(month)2     -0.007044   0.035018  -0.201    0.841    \n#&gt; as.factor(month)3      0.033570   0.035018   0.959    0.338    \n#&gt; as.factor(month)4      0.018785   0.035018   0.536    0.592    \n#&gt; as.factor(month)5     -0.021115   0.035018  -0.603    0.547    \n#&gt; as.factor(month)6      0.009121   0.035018   0.260    0.795    \n#&gt; as.factor(month)7     -0.020344   0.035018  -0.581    0.561    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.6632 on 4270 degrees of freedom\n#&gt; Multiple R-squared:  0.4899, Adjusted R-squared:  0.4859 \n#&gt; F-statistic: 124.3 on 33 and 4270 DF,  p-value: &lt; 2.2e-16\n\n\nas.factor() converts a vector object to a factor. We specify a regressor using as.factor() when the regressor is a categorical variable. We want R to treat the variable as a categorical one instead as a numeric one. In our case, the regressors “Hospital ID” and “Month” are categorical variables, thus we need to use as.factor()."
  },
  {
    "objectID": "content/01_journal/09_iv.html#dag",
    "href": "content/01_journal/09_iv.html#dag",
    "title": "Instrumental Variables",
    "section": "",
    "text": "# Randomized encouragement trial \nrand_enc_dag &lt;- dagify(\n  Y ~ X,\n  Z ~ Y,\n  coords = list(x = c(X = 1, Y = 3, Z = 5),\n                y = c(X = 1, Y = 1, Z = 1)),\n  labels = list(X = \"randomly encouraged\",\n                Y = \"use feature\",\n                Z = \"time spent on app\")\n)\n\n# Plot DAG\nggdag(rand_enc_dag) +\n  theme_dag() +\n  geom_dag_point(color = \"lightblue\") +\n  geom_dag_text(color = \"black\") +\n  geom_dag_edges(edge_color = \"black\") +\n  geom_dag_label_repel(aes(label = label))"
  },
  {
    "objectID": "content/01_journal/09_iv.html#naive-biased-estimate",
    "href": "content/01_journal/09_iv.html#naive-biased-estimate",
    "title": "Instrumental Variables",
    "section": "\n2 Naive, biased estimate",
    "text": "2 Naive, biased estimate\nA naive biased estimate obtained by regressing time_spent on used_ftr.\n\nmodel_naive &lt;- lm(time_spent ~ used_ftr, data = rand_enc)\nsummary(model_naive)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = time_spent ~ used_ftr, data = rand_enc)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -20.4950  -3.5393   0.0158   3.5961  20.5051 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 18.86993    0.06955   271.3   &lt;2e-16 ***\n#&gt; used_ftr    10.82269    0.10888    99.4   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.351 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.497,  Adjusted R-squared:  0.497 \n#&gt; F-statistic:  9881 on 1 and 9998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/09_iv.html#instrumental-variable-estimation",
    "href": "content/01_journal/09_iv.html#instrumental-variable-estimation",
    "title": "Instrumental Variables",
    "section": "\n3 Instrumental variable estimation",
    "text": "3 Instrumental variable estimation\n\n# Compute correlations\ncor(rand_enc) %&gt;% round(2)\n\n#&gt;            rand_enc used_ftr time_spent\n#&gt; rand_enc       1.00     0.20       0.13\n#&gt; used_ftr       0.20     1.00       0.71\n#&gt; time_spent     0.13     0.71       1.00\n\nrand_enc_filtered &lt;- dplyr::filter(rand_enc, used_ftr==0)\ncor(rand_enc_filtered[\"rand_enc\"], rand_enc_filtered[\"time_spent\"]) %&gt;% round(2)\n\n#&gt;          time_spent\n#&gt; rand_enc      -0.02\n\n\nFrom the correlation matrix, we can see that there is a positive correlation between rand_enc and used_ftr, which is also called first-stage and confirms the relevance of our instrument. The randomized encouragement does affect (positively) decision to test the new feature of users. This is assumption “Instrument relevance”. However, since the association between rand_enc and used_ftr is not so strong, the instrument is not so powerful.\nFor the partly testable assumption “Excludability”, the conditional correlation between rand_enc and time_spent given used_ftr suggests a very weak negative linear relationship between rand_enc and time_spent when used_ftr is held constant. The correlation coefficient is close to zero, indicating that the instrument rand_enc influences the outcome of time_spent only through the treatment variable used_ftr.\nIn conclusion, instrumental variable estimation is an adequate procedure for the given situation."
  },
  {
    "objectID": "content/01_journal/09_iv.html#iv-estimate-using-2sls",
    "href": "content/01_journal/09_iv.html#iv-estimate-using-2sls",
    "title": "Instrumental Variables",
    "section": "\n4 IV estimate using 2SLS",
    "text": "4 IV estimate using 2SLS\n\n# First stage\nfirst_stage &lt;- lm(used_ftr ~ rand_enc, data = rand_enc)\nsummary(first_stage)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = used_ftr ~ rand_enc, data = rand_enc)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -0.5071 -0.3062 -0.3062  0.4929  0.6938 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 0.306164   0.006851   44.69   &lt;2e-16 ***\n#&gt; rand_enc    0.200940   0.009624   20.88   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.4811 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.04178,    Adjusted R-squared:  0.04169 \n#&gt; F-statistic:   436 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\n# Predicted 'probabilities' from first stage\npred_fs &lt;- predict(first_stage)\n\n# Create table with predictions and actual decisions\npred_vs_actl &lt;- tibble(\n  pred = pred_fs,\n  actl = rand_enc$used_ftr\n)\n\n# Plot predictions vs original\nggplot(pred_vs_actl, aes(x = pred, y = actl, color = as.factor(actl))) +\n  geom_jitter(alpha = .5) +\n  scale_color_discrete(labels = c(\"Control Group\", \"Treatment Group\")) +\n    theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\n# Second stage\nsecond_stage &lt;- lm(rand_enc$time_spent ~ first_stage$fitted.values)\nsummary(second_stage)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = rand_enc$time_spent ~ first_stage$fitted.values)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -25.8757  -5.4714  -0.3263   5.3807  25.6541 \n#&gt; \n#&gt; Coefficients:\n#&gt;                           Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                19.3124     0.3129   61.72   &lt;2e-16 ***\n#&gt; first_stage$fitted.values   9.7382     0.7447   13.08   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 7.482 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.01681,    Adjusted R-squared:  0.01672 \n#&gt; F-statistic:   171 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\n# Using our instrument (rand_enc), we try to eliminate the bias induced by the omitted variable. If all assumptions regarding the validity of our instrument are met, the resulting coefficient should be close to what we have defined above.\nmodel_iv &lt;- iv_robust(time_spent ~ used_ftr | rand_enc, data = rand_enc)\nsummary(model_iv)\n\n#&gt; \n#&gt; Call:\n#&gt; iv_robust(formula = time_spent ~ used_ftr | rand_enc, data = rand_enc)\n#&gt; \n#&gt; Standard error type:  HC2 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value  Pr(&gt;|t|) CI Lower CI Upper   DF\n#&gt; (Intercept)   19.312     0.2248   85.89 0.000e+00   18.872    19.75 9998\n#&gt; used_ftr       9.738     0.5353   18.19 8.716e-73    8.689    10.79 9998\n#&gt; \n#&gt; Multiple R-squared:  0.4921 ,    Adjusted R-squared:  0.492 \n#&gt; F-statistic:   331 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\nThe naive estimate of used_ftr is \\(10.82269\\) and the IV estimate is \\(9.738\\). Since the naive estimate is \\(1.08469\\) larger than the IV estimate, it is biased and it has an upward bias."
  }
]